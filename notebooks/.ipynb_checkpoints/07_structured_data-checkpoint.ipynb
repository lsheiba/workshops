{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Classifying Structured Data with TensorFlow\n",
    "This notebook demonstrates classifying structured data, like you might find in a CSV file, or a large spreadsheet. The code presented here can become a starting point for a problem you care about. Along the way, we'll introduce feature engineering - which you can use to transform the columns from the CSV into a more useful representation.\n",
    "\n",
    "### Tips \n",
    "\n",
    "* **Delete the checkpoints folder before re-running this notebook.** This notebook uses Estimators. When you run the notebook, they'll write logs and a checkpoint file to *./graphs* (a directory that will be created in the same folder as this notebook on disk). If you'd like to run this notebook multiple times, delete the graphs folder first, so the Estimators begin training from a clean slate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "print('This code requires TensorFlow v1.3+')\n",
    "print('You have:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "\n",
    "Here, we'll work with the [Adult dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/) from the 1990 US Census. Our task is to predict whether an individual has an income over $50,000 / year, based attributes such as their age and occupation. This is a generic problem with a variety of numeric and categorical attributes - which makes it useful for demonstration purposes.\n",
    "\n",
    "A great way to get to know the dataset is by using [Facets](https://github.com/pair-code/facets) - an open source tool for visualizing and exploring data. At the time of writing, the [online demo](https://pair-code.github.io/facets/) has the Census data preloaded. Try it! In the screenshot below, each dot represents a person, or, a row from the CSV. They're colored by the label we want to predict ('blue' for less than 50k / year, 'red' for more). In the online demo, clicking on a person will show the attributes, or columns from the CSV file, that describe them - such as their age and occuptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Image(filename='../images/facets1.jpg', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "census_test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "census_train_path = tf.contrib.keras.utils.get_file('census.train', census_train_url)\n",
    "census_test_path = tf.contrib.keras.utils.get_file('census.test', census_test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is missing a header, so we'll add one here. You can find descriptions of these columns in the [names file](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "  'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "  'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
    "  'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "  'income'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the using Pandas\n",
    "\n",
    "In the first half of this notebook, we'll assume the dataset fits into memory. Should you need to work with larger files, you can use the Datasets API to read them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes\n",
    "# 1) We provide the header from above.\n",
    "# 2) The test file has a line we want to disgard at the top, so we include the parameter 'skiprows=1'\n",
    "census_train = pd.read_csv(census_train_path, index_col=False, names=column_names) \n",
    "census_test = pd.read_csv(census_test_path, skiprows=1, index_col=False, names=column_names) \n",
    "\n",
    "# Drop any rows that have missing elements\n",
    "# Of course there are other ways to handle missing data, but we'll\n",
    "# take the simplest approach here.\n",
    "census_train = census_train.dropna(how=\"any\", axis=0)\n",
    "census_test = census_test.dropna(how=\"any\", axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct formatting problems with the Census data\n",
    "As it happens, there's a small formatting problem with the testing CSV file that we'll fix here. The labels in the testing file are written differently than they are in the training file. Notice the extra \".\" after \"<=50K\" and \">50K\" in the screenshot below.\n",
    "\n",
    "You can open the CSVs in your favorite text editor to see the error, or you can see it with Facets in \"overview mode\" - which makes it easy to catch this kind of mistake early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/facets2.jpg', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the label we want to predict into its own object \n",
    "# At the same time, we'll convert it into true/false to fix the formatting error\n",
    "census_train_label = census_train.pop('income').apply(lambda x: \">50K\" in x)\n",
    "census_test_label = census_test.pop('income').apply(lambda x: \">50K\" in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it useful to print out the shape of the data as I go, as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training examples: %d\" % census_train.shape[0])\n",
    "print (\"Training labels: %d\" % census_train_label.shape[0])\n",
    "print()\n",
    "print (\"Test examples: %d\" % census_test.shape[0])\n",
    "print (\"Test labels: %d\" % census_test_label.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, I like to see the head of each file, to help spot errors early on. First for the training examples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and now for the labels. Notice the label column is now true/false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_train_label.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likewise, you could do a spot check of the testing examples and labels.\n",
    "# census_test.head()\n",
    "# census_test_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators and Input Functions\n",
    "\n",
    "[TensorFlow Estimators](https://www.tensorflow.org/get_started/estimator) provide a high-level API you can use to train your models. Here, we'll use Canned Estimators (\"models-in-a-box\"). These handle many implementation details for you, so you can focus on solving your problem (e.g., by coming up with informative features using the feature engineering techniques we introduce below). \n",
    "\n",
    "To learn more about Estimators, you can watch this talk from Google I/O by Martin Wicke: [Effective TensorFlow for Non-Experts](https://www.youtube.com/watch?v=5DknTFbcGVM). Here's a diagram of the methods we'll use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/estimators1.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably guess the purpose of methods like train / evaluate / and predict. What may be new to you, though, are [Input Functions](https://www.tensorflow.org/get_started/estimator#describe_the_training_input_pipeline). These are responsible for reading your data, preprocessing it, and sending it to the model. When you use an input function, your code will read *estimator.train(your_input_function)* rather than *estimator.train(your_training_data)*, which you may be accustomed to. \n",
    "\n",
    "First, we'll use a [pre-built](https://www.tensorflow.org/get_started/input_fn) input function. This is useful for working with a Pandas dataset that you happen to already have in memory, as we do here. Next, we'll use the [Datasets API](https://www.tensorflow.org/programmers_guide/datasets) to write our own. The Datasets API will become the standard way of writing input functions moving forward. At the time of writing (v1.3), it's in contrib, but will move to core in v1.4. We'll update this notebook after that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input functions for training and testing data\n",
    "Why do we need two input functions? There are a couple differences in how we handle our training and testing data. We want the training input function to loop over the data indefinitely (returning batches of examples and labels when called). We want the testing input function run for just one epoch, so we can make one prediction for each testing example. We'll also want to shuffle the training data, but not the testing data (so we can compare it to the labels later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_input_fn(): \n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=census_train,\n",
    "        y=census_train_label, \n",
    "        batch_size=32,\n",
    "        num_epochs=None, # Repeat forever\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_input_fn():\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x=census_test,\n",
    "        y=census_test_label, \n",
    "        num_epochs=1, # Just one epoch\n",
    "        shuffle=False) # Don't shuffle so we can compare to census_test_labels later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the bottom of the notebook for an example of doing this with the new Datasets API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Now we'll specify the features we'll use and how we'd like them represented. To do so, we'll use tf.feature_columns. Basically, these enable you to represent a column from the CSV file in a variety of interesting ways. Our goal here is to demostrate how to work with different types of features, rather than to aim for an accurate model. Here are five different types we'll use in our Linear model:\n",
    "\n",
    "* A numeric_column. This is just a real-valued attribute.\n",
    "* A bucketized_column. TensorFlow automatically buckets a numeric column for us.\n",
    "* A categorical_column_with_vocabulary_list. This is just a categorical column, where you know the possible values in advance. This is useful when you have a small number of possibilities.\n",
    "* A categorical_column_with_hash_bucket. This is a useful way to represent categorical features when you have a large number of values. Beware of hash collisions.\n",
    "* A crossed_column. Linear models cannot consider interactions between features, so we'll ask TensorFlow to cross features for us.\n",
    "\n",
    "In the Deep model, we'll also use:\n",
    "\n",
    "* An embedding column(!). This automatically creates an embedding for categorical data.\n",
    "\n",
    "You can learn more about feature columns in the [Large Scale Linear Models Tutorial](https://www.tensorflow.org/tutorials/linear#feature_columns_and_transformations) in the [Wide & Deep tutorial](https://www.tensorflow.org/tutorials/wide_and_deep#define_base_feature_columns), as well as in the [API doc](https://www.tensorflow.org/api_docs/python/tf/feature_column). \n",
    "\n",
    "Following is a demo of a couple of the things you can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the feature columns we'll use to train the Linear model\n",
    "feature_columns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start, we'll use the raw, numeric value of age.\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "feature_columns.append(age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll add a bucketized column. Bucketing divides the data based on ranges, so the classifier can consider each independently. This is especially helpful to linear models. Here's what the buckets below look like for age, as seen using Facets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/buckets.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column('age'), \n",
    "    boundaries=[31, 46, 60, 75, 90] # specify the ranges\n",
    ")\n",
    "\n",
    "feature_columns.append(age_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also evenly divide the data, if you prefer not to specify the ranges yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_buckets = tf.feature_column.bucketized_column(\n",
    "#    tf.feature_column.numeric_column('age'), \n",
    "#    list(range(10))\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a categorical column\n",
    "# We're specifying the possible values\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"education\", [\n",
    "        \"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\",\n",
    "        \"Some-college\", \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\",\n",
    "        \"Doctorate\", \"Prof-school\", \"5th-6th\", \"10th\", \"1st-4th\",\n",
    "        \"Preschool\", \"12th\"\n",
    "    ])\n",
    "\n",
    "feature_columns.append(education)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer not to specify the vocab in code, you can also read it from a file, or alternatively - use a categorical_column_with_hash_bucket. Beware of hash collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A categorical feature with a possibly large number of values\n",
    "# and the vocabulary not specified in advance.\n",
    "native_country = tf.feature_column.categorical_column_with_hash_bucket('native-country', 1000)\n",
    "feature_columns.append(native_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a crossed column for age and education. Here's what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/crossed.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cross_education = tf.feature_column.crossed_column(\n",
    "    [age_buckets, education],\n",
    "    hash_bucket_size=int(1e4) # Using a hash is handy here\n",
    ")\n",
    "feature_columns.append(age_cross_education)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Canned Linear Estimator\n",
    "\n",
    "Note: logs and a checkpoint file will be written to *model_dir*. Delete this from disk before rerunning the notebook for a clean start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = create_train_input_fn()\n",
    "estimator = tf.estimator.LinearClassifier(feature_columns, model_dir='graphs/linear', n_classes=2)\n",
    "estimator.train(train_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = create_test_input_fn()\n",
    "estimator.evaluate(test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "The Estimator returns a generator object. This bit of code demonstrates how to retrieve predictions for individual examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize the input function\n",
    "test_input_fn = create_test_input_fn()\n",
    "\n",
    "predictions = estimator.predict(test_input_fn)\n",
    "i = 0\n",
    "for prediction in predictions:\n",
    "    true_label = census_test_label[i]\n",
    "    predicted_label = prediction['class_ids'][0]\n",
    "    # Uncomment the following line to see probabilities for individual classes\n",
    "    # print(prediction) \n",
    "    print(\"Example %d. Actual: %d, Predicted: %d\" % (i, true_label, predicted_label))\n",
    "    i += 1\n",
    "    if i == 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What features can you use to achieve higher accuracy?\n",
    "This dataset is imbalanced, so an an accuracy of around 75% is low in this context (this could be achieved merely by predicting *everyone* makes less than 50k / year). In fact, if you look through the predictions closely, you'll find that many are zero. We'll get a little smarter as we go. (Note: We haven't tried to optimize for accuracy on this dataset - the goal here is to demonstrate different feature engineering techniques you can explore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Deep Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an embedding feature(!) and update the feature columns\n",
    "Instead of using a hash to represent categorical features, here we'll use a learned embedding. (Cool, right?) We'll also update how the features are represented for our deep model. Here, we'll use a different combination of features that before, just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll provide vocabulary lists for features with just a few terms\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'workclass',\n",
    "    [' Self-emp-not-inc', ' Private', ' State-gov', ' Federal-gov',\n",
    "     ' Local-gov', ' ?', ' Self-emp-inc', ' Without-pay', ' Never-worked'])\n",
    "\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'education',\n",
    "    [' Bachelors', ' HS-grad', ' 11th', ' Masters', ' 9th', ' Some-college',\n",
    "     ' Assoc-acdm', ' Assoc-voc', ' 7th-8th', ' Doctorate', ' Prof-school',\n",
    "     ' 5th-6th', ' 10th', ' 1st-4th', ' Preschool', ' 12th'])\n",
    "\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'marital-status',\n",
    "    [' Married-civ-spouse', ' Divorced', ' Married-spouse-absent',\n",
    "     ' Never-married', ' Separated', ' Married-AF-spouse', ' Widowed'])\n",
    "     \n",
    "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'relationship',\n",
    "    [' Husband', ' Not-in-family', ' Wife', ' Own-child', ' Unmarried',\n",
    "     ' Other-relative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "\n",
    "    # Use indicator columns for low dimensional vocabularies\n",
    "    tf.feature_column.indicator_column(workclass),\n",
    "    tf.feature_column.indicator_column(education),\n",
    "    tf.feature_column.indicator_column(marital_status),\n",
    "    tf.feature_column.indicator_column(relationship),\n",
    "\n",
    "    # Use embedding columns for high dimensional vocabularies\n",
    "    tf.feature_column.embedding_column(  # now using embedding!\n",
    "        # params are hash buckets, embedding size\n",
    "        tf.feature_column.categorical_column_with_hash_bucket('occupation', 100), 10),\n",
    "    \n",
    "    # numeric features\n",
    "    tf.feature_column.numeric_column('age'),\n",
    "    tf.feature_column.numeric_column('education-num'),\n",
    "    tf.feature_column.numeric_column('capital-gain'),\n",
    "    tf.feature_column.numeric_column('capital-loss'),\n",
    "    tf.feature_column.numeric_column('hours-per-week'),   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELDIR = os.environ['TRAINING_DIR']+os.sep+'graphs/dnn'\n",
    "estimator = tf.estimator.DNNClassifier(hidden_units=[256, 128, 64], \n",
    "                                       feature_columns=feature_columns, \n",
    "                                       n_classes=2, \n",
    "                                       model_dir=MODELDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = create_train_input_fn()\n",
    "estimator.train(train_input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = create_test_input_fn()\n",
    "estimator.evaluate(test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "If you like, you can start TensorBoard by running this from a terminal command (in the same directory as this notebook):\n",
    "\n",
    "```$ tensorboard --logdir=graphs```\n",
    "\n",
    "then pointing your web-browser to ```http://localhost:6006``` (check the TensorBoard output in the terminal in case it's running on a different port).\n",
    "\n",
    "When that launches, you'll be able to see a variety of graphs that compares the linear and deep models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../images/tensorboard.jpeg', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets API\n",
    "Here, I'll demonstrate how to use the new [Datasets API](https://www.tensorflow.org/programmers_guide/datasets), which you can use to write complex input pipeline from simple, reusable pieces. \n",
    "\n",
    "At the time of writing (v1.3) this API is in contrib. It's most likely moving into core in v1.4, which is good news. Using TensorFlow 1.4, the below can be written using *regular* Python code to parse the CSV file, via the *Datasets.from_generator()* method. This improves producivity a lot - it means you can use Python to read, parse, and apply whatever logic you wish to your input data - then you can take advantage of the reusable pieces of the Datasets API (e.g., batch, shuffle, repeat, etc) - as well as the optional performance tuning (e.g., prefetch, parallel process, etc).\n",
    "\n",
    "In combination with Estimators, this means you can train and tune deep models at scale on data of almost any size, entirely using a high-level API. I'll update this notebook after v1.4 is released with an example. It's neat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to reset the notebook to show you how to do this from a clean slate\n",
    "%reset -f \n",
    "\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "census_train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "census_test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "census_train_path = tf.contrib.keras.utils.get_file('census.train', census_train_url)\n",
    "census_test_path = tf.contrib.keras.utils.get_file('census.test', census_test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide default values for each of the CSV columns\n",
    "# and a header at the same time.\n",
    "csv_defaults = collections.OrderedDict([\n",
    "  ('age',[0]),\n",
    "  ('workclass',['']),\n",
    "  ('fnlwgt',[0]),\n",
    "  ('education',['']),\n",
    "  ('education-num',[0]),\n",
    "  ('marital-status',['']),\n",
    "  ('occupation',['']),\n",
    "  ('relationship',['']),\n",
    "  ('race',['']),\n",
    "  ('sex',['']),\n",
    "  ('capital-gain',[0]),\n",
    "  ('capital-loss',[0]),\n",
    "  ('hours-per-week',[0]),\n",
    "  ('native-country',['']),\n",
    "  ('income',['']),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode a line from the CSV.\n",
    "def csv_decoder(line):\n",
    "    \"\"\"Convert a CSV row to a dictonary of features.\"\"\"\n",
    "    parsed = tf.decode_csv(line, list(csv_defaults.values()))\n",
    "    return dict(zip(csv_defaults.keys(), parsed))\n",
    "\n",
    "# The train file has an extra empty line at the end.\n",
    "# We'll use this method to filter that out.\n",
    "def filter_empty_lines(line):\n",
    "    return tf.not_equal(tf.size(tf.string_split([line], ',').values), 0)\n",
    "\n",
    "def create_train_input_fn(path):\n",
    "    def input_fn():    \n",
    "        dataset = (\n",
    "            tf.contrib.data.TextLineDataset(path)  # create a dataset from a file\n",
    "                .filter(filter_empty_lines)  # ignore empty lines\n",
    "                .map(csv_decoder)  # parse each row\n",
    "                .shuffle(buffer_size=1000)  # shuffle the dataset\n",
    "                .repeat()  # repeate indefinitely\n",
    "                .batch(32)) # batch the data\n",
    "\n",
    "        # create iterator\n",
    "        columns = dataset.make_one_shot_iterator().get_next()\n",
    "        \n",
    "        # separate the label and convert it to true/false\n",
    "        income = tf.equal(columns.pop('income'),\" >50K\") \n",
    "        return columns, income\n",
    "    return input_fn\n",
    "\n",
    "def create_test_input_fn(path):\n",
    "    def input_fn():    \n",
    "        dataset = (\n",
    "            tf.contrib.data.TextLineDataset(path)\n",
    "                .skip(1) # The test file has a strange first line, we want to ignore this.\n",
    "                .filter(filter_empty_lines)\n",
    "                .map(csv_decoder)\n",
    "                .batch(32))\n",
    "\n",
    "        # create iterator\n",
    "        columns = dataset.make_one_shot_iterator().get_next()\n",
    "        \n",
    "        # separate the label and convert it to true/false\n",
    "        income = tf.equal(columns.pop('income'),\" >50K\") \n",
    "        return columns, income\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's code you can use test the Dataset input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = create_train_input_fn(census_train_path)\n",
    "next_batch = train_input_fn()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    features, label = sess.run(next_batch)\n",
    "    print(features['education'])\n",
    "    print(label)\n",
    "\n",
    "    print()\n",
    "\n",
    "    features, label = sess.run(next_batch)\n",
    "    print(features['education'])\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you can use the input functions to train and evaluate your Estimators. I'll add some minimal code to do this, just to show the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = create_train_input_fn(census_train_path)\n",
    "test_input_fn = create_train_input_fn(census_test_path)\n",
    "\n",
    "feature_columns = [\n",
    "    tf.feature_column.numeric_column('age'),\n",
    "]\n",
    "\n",
    "MODELDIR = os.environ['TRAINING_DIR']+os.sep+'graphs_dataset/canned'\n",
    "estimator = tf.estimator.DNNClassifier(hidden_units=[256, 128, 64], \n",
    "                                       feature_columns=feature_columns, \n",
    "                                       n_classes=2, \n",
    "                                       # creating a new folder in case you haven't cleared \n",
    "                                       # the old one yet\n",
    "                                       model_dir=MODELDIR)\n",
    "\n",
    "estimator.train(train_input_fn, steps=100)\n",
    "estimator.evaluate(train_input_fn, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be a good time to clean up the logs and checkpoints on disk, by deleting ```./graphs``` and ```./graphs_datasets```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "## Learn more about Feature Engineering\n",
    "\n",
    "Check out the [Wide and Deep tutorial](https://www.tensorflow.org/tutorials/wide_and_deep) which shows how to combine a Linear Classifier and Deep Neural Network, so you can take advantage of the best features for each. Bonus: that tutorial contains another kind of Estimator you can use.\n",
    "\n",
    "## Learn more about Datasets\n",
    "\n",
    "Check out the [programmers guide](https://www.tensorflow.org/programmers_guide/datasets), and check back after v1.4 is released for the Dataset.from_generator method, which I think will improve productivity a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
